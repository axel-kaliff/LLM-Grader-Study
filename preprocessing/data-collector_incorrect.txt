Logical Error - The `batch_save_file_contents()` and `batch_crawl_repos()` methods in the `FileFetcher` class had incorrect arguments passed when calling other methods. 

---

```
import requests
import base64
from io import StringIO
import pandas as pd

# Define Github API endpoint and access token
api_endpoint = "https://api.github.com"
access_token = "YOUR_ACCESS_TOKEN"

# Define repository and file path
repo_name = "org/repo"
file_path = "path/to/file"

# Define headers with access token
headers = {"Authorization": f"Token {access_token}"}

# Fetch file contents using Github API
response = requests.get(f"{api_endpoint}/repos/{repo_name}/contents/{file_path}", headers=headers)
response.raise_for_status()

# Decode file contents from base64 and save as txt file
content = base64.b64decode(response.json()["content"]).decode()
with open(f"{file_path}.txt", "w") as f:
    f.write(content)

# createa a datatype to represent a github repo with a name and a list of files
class Repo:
    def __init__(self, name, files):
        self.name = name
        self.files = files

class FileFetcher:

    def __init__(self, access_token, api_endpoint="https://api.github.com"):
        self.api_endpoint = api_endpoint
        self.access_token = access_token

    def get_file_contents(self, repo_name, file_path):
        headers = {"Authorization": f"Token {self.access_token}"}
        response = requests.get(f"{self.api_endpoint}/repos/{repo_name}/contents/{file_path}", headers=headers)
        response.raise_for_status()
        return base64.b64decode(response.json()["content"]).decode()

    def save_file_contents(self, repo, file_path):
        content = self.get_file_contents(repo.name, file_path)
        with open(f"{file_path}.txt", "w") as f:
            f.write(content)

    def batch_save_file_contents(self, repo):
        for file_path in repo.files:
            self.save_file_contents(repo, file_path)
    
    def batch_crawl_repos(self, repos):
        for repo in repos:
            self.batch_save_file_contents(repo)
    
    def get_repos(self, org_name, keywords):
        all_repos = []
        for keyword in keywords:
            response = requests.get(f"{self.api_endpoint}/orgs/{org_name}/repos", headers=headers)
            response.raise_for_status()
            repos = [repo["name"] for repo in response.json() if keyword in repo["name"]]
            all_repos.extend(repos)
        return all_repos 


# Example usage
fetcher = FileFetcher(access_token)

org_name = "kth-15"
keywords = ["task-12", "task-13", "task-14"]

repo_names = fetcher.get_repos("org", keywords)
# files for task 12, 13, 14 respectively
files = [ 
    ["src/LinkedList.java", "src/LinkedListTest.java"],
    ["src/binarySearchTree.java", "src/binarySearchTreeTest.java", "docs/answers.md"],
    ["src/bfs.java", "src/bfsTest.java", "docs/answers.md"]
]

# create a list of Repo objects
repos = [Repo(repo_name, files) for repo_name, files in zip(repo_names, files)]


# fetch and save files for all repos
fetcher.batch_crawl_repos(repos)
``` 

This code now correctly crawls Github for the correct repos and files with the given keywords, and saves them to text files.